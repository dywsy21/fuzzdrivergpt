import os
import re
import uuid
import copy
import json
import time
import utils
import docker
import shutil
import argparse
import traceback

import queue
import threading

from libTarget import TargetCfg
from libValidator import BaseValidator

from ipdb import launch_ipdb_on_exception

import cfgs

import logging
logger = logging.getLogger(__name__)

docker_client = docker.from_env()
g_task_id = ''

def docker_wait_and_kill(lock, docker_ref, remove):
	try:
		with lock:
			docker_ref.reload()
		rets = docker_ref.wait(timeout=120)
		if rets['StatusCode'] != 0:
			logger.warning('container does not start as expected, check the logs:\n%s' % (docker_ref.logs()))
			raise Exception('starting container meets error')

	except Exception as ex:
		# sometimes the fuzzer will stuck, we kill it after passing a safe & long time period
		#logger.error('jump out when timeout, we will kill it soon')
		docker_ref.kill()

	finally:
		if remove:
			with lock:
				docker_ref.remove(force=True)

	#logger.debug('[1] status: %s' % (docker_ref.status))
	#logger.debug('[2] status: %s' % (docker_client.containers.list(all=True, filters={'id':docker_ref.id})[0].status))

class ContainerValidator(BaseValidator):

	def __init__(self, cfg, lock):
		super().__init__(cfg)
		self.lock = lock
		self.docker = None

	def reset_fuzz(self):
		if self.docker != None:
			try:
				with self.lock:
					self.docker.remove(force=True)
			except Exception as e:
				# it is okay to eat errors of remove here
				#logger.debug('meet exception when kill docker: %s' % (e))
				pass
			finally:
				self.docker = None
		if os.path.exists(self.cfg.workdir):
			# clean all rubbish files generated by fuzz drivers
			shutil.rmtree(self.cfg.workdir, ignore_errors=True)
			os.makedirs(self.cfg.workdir, exist_ok=True)

		self.reset()

	def get_vali_status(self):
		status = {}

		if self.docker == None:
			# has not started container
			status['build_cmd'] = self.cfg.buildcmd
			status['run_cmd'] = self.cfg.runcmd
			status['vali_phase'] = 'init'
			status["vali_detail"] = "Has not started containered validation..."

		else:
			if not os.path.exists(self.cfg.statusfile):
				status['build_cmd'] = self.cfg.buildcmd
				status['run_cmd'] = self.cfg.runcmd
				status['vali_phase'] = 'init'
				status["vali_detail"] = "Container starting..."

			else:
				with open(self.cfg.statusfile, 'r') as f:
					status = json.load(f)

					if (status['error_detected'] == False) and (status['vali_phase'] == 'link'):
						# this indicates that the validation is in the fuzzing phase, in this case, we get the log of fuzzing for reporting the fuzzing status to user
						status["vali_detail"] = self.show_fuzz_result()

		status['code'] = self.show_code()
		return status

	def validate_wrap(self, vid, query_result, fuzz, check_fuzz_fns, sema, sync):
		global debug, docker_client

		# prepare & start a docker
		TargetCfg.pickleTo(self.cfg, self.cfg.validatepickle)
		dockercmd = 'python3 /root/workspace/fuzzdrivergpt/libValidator.py %s %s %s %s %s' % (self.cfg.validatepickle, vid, fuzz, check_fuzz_fns, sema)
		if debug:
			logger.debug(dockercmd)
			dockercmd = 'sleep 50h'
		imagename = self.cfg.imagename
		dockervolumns = [ 
			#'%s/targets/%s:/root/workspace/fuzzdrivergpt/install' % (cfgs.FDGPT_DIR, self.cfg.target),
			'%s:%s' % (self.cfg.workdir, self.cfg.workdir),
			'%s:%s' % (self.cfg.cachedir, self.cfg.cachedir),
			'%s/libTarget.py:/root/workspace/fuzzdrivergpt/libTarget.py' % (cfgs.FDGPT_DIR),
			'%s/libVR.py:/root/workspace/fuzzdrivergpt/libVR.py' % (cfgs.FDGPT_DIR),
			'%s/libValidator.py:/root/workspace/fuzzdrivergpt/libValidator.py' % (cfgs.FDGPT_DIR),
			'%s/libSemanticChecker.py:/root/workspace/fuzzdrivergpt/libSemanticChecker.py' % (cfgs.FDGPT_DIR),
			'%s/meta/benchapidata.json:/root/workspace/fuzzdrivergpt/meta/benchapidata.json' % (cfgs.FDGPT_DIR),
			'%s/cfgs.py:/root/workspace/fuzzdrivergpt/cfgs.py' % (cfgs.FDGPT_DIR),
		]
		env = [
			'FUZZING_ENGINE=libfuzzer',
			'SANITIZER=address',
			'ARCHITECTURE=x86_64',
			'PROJECT_NAME=%s' % (self.cfg.target),
			'HELPER=True',
			# TODO: language should be parameterized
			'FUZZING_LANGUAGE=c',
		]
		workdir = self.cfg.workdir

		#logger.debug('workdir is %s' % (self.cfg.workdir))
		#logger.debug('outfile is %s' % (self.cfg.outfile))
		#logger.debug('query_result is %s' % (query_result))

		self.update_code(query_result)

		# run a docker
		try:
			with self.lock:
				self.docker = docker_client.containers.run(imagename, command=dockercmd, volumes=dockervolumns, working_dir=workdir, privileged=True, remove=False, detach=True, network_mode='none')
				#self.docker = docker_client.containers.run(imagename, command=dockercmd, volumes=dockervolumns, working_dir=workdir, privileged=True, remove=False, detach=True)
				self.docker.reload()

		except Exception as ex:
			logger.error('meet exception when run docker %s' % (ex))
			raise ex

		#logger.debug('docker logs %s' % (self.docker.logs()))
		if debug:
			logger.debug('docker exec -it %s bash' % (self.docker.name))
			os._exit(0)

		if sync:
			docker_wait_and_kill(self.lock, self.docker, remove=False)
			# retrieve result
			status = self.get_vali_status()
			with self.lock:
				self.docker.remove(force=True)
			self.del_if_exist(self.cfg.validatepickle)
			return status

		else:
			threading.Thread(target=lambda a: docker_wait_and_kill(self.lock, a, remove=True), args=([self.docker])).start()
			return None

rslts = None
debug = False

def validate_do_func_debug_wrapper(*argv):
	try:
		return validate_do_func(*argv)
	except Exception as e:
		raise Exception('Catched Exception Stack:\n%s' % (traceback.format_exc()))

def validate_do_func(task_idx, lock, args):
	global g_task_id, rslts

	ridx, sidxs = args[0], args[1]
	rslt = rslts[ridx]

	logger.info('[-] handling query id %s with solutions %s' % (rslt['id'], sidxs))

	qid = rslt['id']
	query = rslt['query']
	target = rslt['target']
	language = rslt['language']
	buildyml = 'yml/' + language + '.yml'
	sema = False
	if 'semantic_check' in rslt:
		sema = rslt['semantic_check']
	check_fuzz_fns = False
	if 'check_fuzz_fns' in rslt:
		check_fuzz_fns = rslt['check_fuzz_fns']

	#logger.debug('lock %s' % (lock))
	validator = ContainerValidator(TargetCfg(basedir=cfgs.FDGPT_WORKDIR, build_cfgs_yml=buildyml, target=target, task_idx=('%s%s' % (g_task_id, task_idx))), lock)

	vali_rslt = {}

	for sidx in sidxs:
		#logger.debug('[-] solution %s' % (sidx))
		logger.debug('[-] handling query id %s solution %s' % (rslt['id'], sidx))

		solution = rslt['result']['solutions'][sidx]

		included = rslt['naiveIncluded'] if 'naiveIncluded' in rslt else None
		query_result = validator.concat_queried_code(query, solution, included=included)

		validator.reset_fuzz()

		sig = rslt['info']['mangled_name']
		validation = validator.validate_wrap(sig, query_result, fuzz=True, check_fuzz_fns=check_fuzz_fns, sema=sema, sync=True)

		validator.reset_fuzz()

		vali_rslt[(ridx, sidx)] = validation

	#logger.debug('[-] finish handling query id %s' % (rslt['id']))
	return vali_rslt

def run_cmdline_validation(querylist, scope, apipattern, idxpattern, valinumperdocker, debug, sequential, para, dockerpara, idxset=None):
	global rslts 

	rslts = querylist

	all_args = []
	all_queries = []
	for ridx in list(range(len(rslts))):
		if idxset != None and ridx not in idxset:
			continue

		if re.search(apipattern, rslts[ridx]['id']) != None:
			if 'ValidateMarkedOnly' in rslts[ridx]:
				if rslts[ridx]['ValidateMarkedOnly'] != 'Marked':
					logger.debug('%s requires validate marked only with "%s", skip its validation' % (rslts[ridx]['id'], rslts[ridx]['ValidateMarkedOnly']))
					continue

			logger.debug('%s is in scope' % (rslts[ridx]['id']))
			if 'solutions' not in rslts[ridx]['result']:
				logger.warning('%s does not have solutions' % (rslts[ridx]['id']))
				rslts[ridx]['result']['solutions'] = []

			all_queries.append(ridx)

			total_solutions = len(rslts[ridx]['result']['solutions'])
			rslts[ridx]['result']['validations'] = total_solutions * [ None ]

			# here can control how many solutions will be validated using the same env for one task
			if valinumperdocker == 0:
				all_args.append( (ridx, [ sidx for sidx in range(0, total_solutions) if re.search(idxpattern, str(sidx)) != None ]) )
			else:
				sidxlist = [ sidx for sidx in range(0, total_solutions) if re.search(idxpattern, str(sidx)) != None ]

				for i in range(0, len(sidxlist), valinumperdocker):
					start, stop = i, i + valinumperdocker
					all_args.append( (ridx, sidxlist[start:stop]) )

	if debug and len(all_queries) > 1:
		logger.error('In debug mode, only one query should be specified')
		exit(1)

	logger.info('In scope %s with pattern `%s`, there are %s tasks, %s solutions' % (scope, apipattern, len(all_queries), len(all_args)))

	def rslt_handle(validated_rslts):
		global rslts

		validated_ridxs = set([])

		# merge validation results
		for validated_rslt in validated_rslts:
			for k, v in validated_rslt.items():
				ridx, sidx = k
				validation = v
				rslts[ridx]['result']['validations'][sidx] = validation

				validated_ridxs.add(ridx)

		# remove unvalidated solutions in list
		for ridx in validated_ridxs:
			solutions = rslts[ridx]['result']['solutions']
			validations = rslts[ridx]['result']['validations']

			if None in validations:
				new_solutions = []
				new_validations = []

				for sidx in range(0, len(solutions)):
					solution = solutions[sidx]
					validation = validations[sidx]
					if validation != None:
						new_solutions.append(solution)
						new_validations.append(validation)

				rslts[ridx]['result']['solutions'] = new_solutions
				rslts[ridx]['result']['validations'] = new_validations

	def interpret_func(task_idx, args):
		global rslts
		ridx, sidxs = args[0], args[1]
		rslt = rslts[ridx]
		return 'idx %s , query id %s, solution idxs %s' % (task_idx, rslt['id'], sidxs)

	if debug or sequential:
		utils.do_in_parallel_with_idx(validate_do_func, all_args, rslt_handle, debug=True, interpret_xargs_func=interpret_func)
	else:
		utils.do_in_parallel_with_idx(validate_do_func_debug_wrapper, all_args, rslt_handle, debug=False, sema=dockerpara, para=para, interpret_xargs_func=interpret_func)
	
	tmp = rslts
	# clean the rslts
	rslts = None

	return tmp

def run_server_mode(ip, port, para, dockerpara, projectpara, valinumperdocker):
	global docker_client

	import ssl
	certfile, keyfile = './manual-validation/frontend/cert.pem', './manual-validation/frontend/key.pem'
	ssl_context = ssl.SSLContext()
	ssl_context.load_cert_chain(certfile, keyfile)

	from flask import Flask, request
	import flask
	from flask_cors import CORS
	from flask_httpauth import HTTPTokenAuth

	app = Flask(__name__)
	CORS(app)
	auth = HTTPTokenAuth(scheme='Bearer')

	tokens = {
		'962c90d2725e51c4c023b532057f2d5c': "authed-user"
	}

	@auth.verify_token
	def verify_token(token):
		if token in tokens:
			return tokens[token]

	Q = queue.Queue()
	taskLock = threading.Lock()
	task_statuses = {}

	# curl https://127.0.0.1:9505/validate --insecure -H "Authorization: Bearer 962c90d2725e51c4c023b532057f2d5c"
	@app.route('/validate', methods=["POST"])
	@auth.login_required
	def validate():
		logger.debug("add new validation task")
		if request.method == "POST":
			received_data = request.get_json()

			task = {}

			id = str(uuid.uuid4())
			inputjson = received_data['inputjson']
			outputjson = received_data['outputjson']
			taskid = '%s-%s-%s' % (inputjson, outputjson, id)

			task['taskid'] = taskid
			task['inputjson'] = inputjson
			task['outputjson'] = outputjson

			Q.put(task)

			with taskLock:
				task_statuses[taskid] = 'inqueue'
			
			return flask.Response(response=json.dumps({ 'taskid' : taskid }), status=200)

	@app.route('/full_status', methods=["GET"])
	@auth.login_required
	def fullstatus():
		logger.debug("dump all task statuses")
		if request.method == "GET":
			full_status = None
			with taskLock:
				full_status = copy.deepcopy(task_statuses)

			return flask.Response(response=json.dumps(full_status), status=200)

	@app.route('/status', methods=["POST"])
	@auth.login_required
	def status():
		logger.debug("check task")
		if request.method == "POST":
			received_data = request.get_json()
			taskid = received_data['taskid']

			status = 'unknown'
			with taskLock:
				if taskid in task_statuses:
					status = task_statuses[taskid]

			return flask.Response(response=json.dumps({ 'status' : status }), status=200)

	# run main task loop
	def main_task_loop():
		while True:
			# accept new incoming task
			# every task is executed sequentially
			item = Q.get()
			try: 
				taskid = item['taskid']
				inputjson = item['inputjson']
				outputjson = item['outputjson']
				logger.info('handling task %s with input %s output %s' % (taskid, inputjson, outputjson))

				# update the task status
				with taskLock:
					task_statuses[taskid] = 'validating'

				# do the task
				querylist = None
				with open(inputjson, 'r') as f:
					querylist = json.load(f)
			
				proj2idxs = {}
				for idx in range(len(querylist)):
					query = querylist[idx]
					project = query['target']
					proj2idxs.setdefault(project, set([]))
					proj2idxs[project].add(idx)
			
				# at most projectpara projects are validated in parallel
				vali_projs_list = []
				proj_list = list(proj2idxs.keys())
				for i in range(0, len(proj_list), projectpara):
					vali_projs_list.append( proj_list[i: i+projectpara] )
				
				logger.info('%s projects will be validated in %s groups' % (len(proj_list), len(vali_projs_list)))

				dict_rslts = {}

				for vali_projs in vali_projs_list:
					## - load the docker image
					#logger.info('loading images of %s' % (vali_projs))
					#for proj in vali_projs:
					#	with open('/root/workspace/dockerenv/images/fuzzdrivergpt/%s_env' % (proj), 'rb') as f:
					#		docker_client.images.load(f.read(-1))

					# - run the validation
					idxset = set([])
					for proj in vali_projs:
						idxset |= proj2idxs[proj]

					logger.info('validating %s' % (vali_projs))
					outobj = run_cmdline_validation(querylist, taskid, '.*', '.*', valinumperdocker, False, False, para, dockerpara, idxset=idxset)

					# - save the results
					for rslt in outobj:
						dict_rslts[rslt['id']] = rslt

					## - remove the docker image
					#logger.info('removing images of %s' % (vali_projs))
					#for proj in vali_projs:
					#	docker_client.images.remove('fuzzdrivergpt/%s:env' % (proj))

				with open(outputjson, 'w') as f:
					json.dump(dict_rslts, f, indent=2, sort_keys=True)

				# update the task status
				with taskLock:
					task_statuses[taskid] = 'done'
			except Exception as e:
				logger.error('task %s meets err: %s' % (taskid, str(e)))
				logger.error('\n'.join(traceback.format_exception(type(e), e, e.__traceback__)))
				with taskLock:
					task_statuses[taskid] = 'err-%s' % (str(e))

			logger.info('task %s is done' % (taskid))
			Q.task_done()

	mainloop = threading.Thread(target=main_task_loop)
	mainloop.start()

	logger.info('=== start running HTTP server ===')
	app.run(ip, port, ssl_context=ssl_context)

def main():
	global g_task_id, debug, rslts

	# parse args
	parser = argparse.ArgumentParser(description='hahaha')
	#
	# specifying the validation
	#
	parser.add_argument('-s', '--scope', required=False, default='ALL', type=str, help='scope can be a target project or ALL')
	parser.add_argument('-a', '--apipattern', required=False, default='.*', type=str, help='the apis whose id (lang-proj-api) matches the api patterns will be evaluated')
	parser.add_argument('-n', '--valinumperdocker', required=False, default=0, type=int, help='validate each fuzz driver in one docker')
	parser.add_argument('-i', '--idxpattern', required=False, default='.*', type=str, help='the fuzz driver whose index matches the pattern will be evaluated')
	#
	# controlling the validation mode
	#
	parser.add_argument('-D', '--debug', required=False, action='store_true', help='debug mode')
	parser.add_argument('-S', '--sequential', required=False, action='store_true', help='validate in sequential, not parallel')
	#
	# controlling the validation service
	#
	parser.add_argument('--servermode', required=False, default=False, action='store_true', help='run validation in servermode')
	parser.add_argument('--ip', required=False, default='0.0.0.0', type=str, help='the IP listened in servermode')
	parser.add_argument('--port', required=False, default=9505, type=int, help='the port listened in servermode')
	#
	# controlling the concurrency (both servermode and non-servermode)
	#
	parser.add_argument('-T', '--taskid', required=False, default='', type=str, help='task id for the whole work, which will be used in the validation working directory name')
	parser.add_argument('-j', '--paranum', required=False, default=0, type=int, help='the apis whose id (lang-proj-api) matches the api patterns will be evaluated, 0 means ncpu')
	parser.add_argument('-dj', '--dockerpara', required=False, default=16, type=int, help='specify the maximum concurrent number of docker api requests, 0 means ncpu')
	parser.add_argument('-pj', '--projectpara', required=False, default=5, type=int, help='specify the maximum concurrent number of projects being validated, the value of this number is limited by the size of docker data-root (sometimes it is in ramdisk')


	# in/out args used by non-servermode
	parser.add_argument('input', nargs='?', help='the json output containing queries, i.e., json output from our batch query plugin')
	parser.add_argument('-o', '--output', required=False, default='', type=str, help='the validated json output file, if not specified, INPUT_validated_SCOPE.json will be used')

	args = parser.parse_args()

	# prepare arg-related vars
	inputjson = args.input
	taskid = args.taskid
	scope = args.scope
	apipattern = args.apipattern
	idxpattern = args.idxpattern
	valinumperdocker = args.valinumperdocker
	debug = args.debug
	sequential = args.sequential
	servermode = args.servermode
	ip = args.ip
	port = args.port
	para = args.paranum if args.paranum != 0 else utils.cpunum()
	dockerpara = args.dockerpara if args.dockerpara != 0  else utils.cpunum()
	projectpara = args.projectpara
	outfile = args.output

	if servermode: 
		if debug or sequential:
			logger.error('server mode does not support debug or sequential mode')
			exit(1)
	else:
		if inputjson == None:
			logger.error('input json file is required')
			exit(1)

	logger.info('inputjson: %s' % (inputjson))
	logger.info('taskid: %s' % (taskid))
	logger.info('scope: %s' % (scope))
	logger.info('apipattern: %s' % (apipattern))
	logger.info('idxpattern: %s' % (idxpattern))
	logger.info('debug mode: %s' % ('on' if debug else 'off'))
	logger.info('sequential mode: %s' % ('on' if sequential else 'off'))
	logger.info('server mode: %s' % ('on' if servermode else 'off'))
	if servermode:
		logger.info('server ip: %s' % (ip))
		logger.info('server port: %s' % (port))
	logger.info('parallel number if not sequential: %s' % ('ncpu' if para == 0 else para))
	logger.info('maximum concurrent number of docker API request: %s' % ('ncpu' if dockerpara == 0 else dockerpara))
	logger.info('validation number per docker: %s' % (valinumperdocker))
	logger.info('maximum concurrent number of projects being validated: %s' % (projectpara))
	if outfile != '':
		logger.info('output file: %s' % (outfile))

	#
	# main logic
	#

	g_task_id = taskid

	if servermode:
		# server mode logic
		run_server_mode(ip, port, para, dockerpara, projectpara, valinumperdocker)
	else:
		# non-server mode logic
		# TODO: currently we don't need to care project concurrency since we assume the project is validated one by one
		querylist = None
		with open(inputjson, 'r') as f:
			querylist = json.load(f)

		outobj = run_cmdline_validation(querylist, scope, apipattern, idxpattern, valinumperdocker, debug, sequential, para, dockerpara, idxset=None)

		dict_rslts = {}
		for rslt in outobj:
			dict_rslts[rslt['id']] = rslt

		outputjson = outfile
		if outputjson == '':
			outputjson = inputjson + '_validated_%s.json' % (scope)
		with open(outputjson, 'w') as f:
			json.dump(dict_rslts, f, indent=2, sort_keys=True)

if __name__ == '__main__':
	#with launch_ipdb_on_exception():
	#	main()
	main()
